{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6358196,"sourceType":"datasetVersion","datasetId":3579787}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1 - setup\n!pip install -q librosa==0.10.0 soundfile==0.12.1 tqdm\n\nimport os, random, math, warnings\nimport numpy as np, pandas as pd\nfrom tqdm import tqdm\nimport librosa, soundfile as sf\nimport matplotlib.pyplot as plt\n\n# TensorFlow / Keras\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Masking, LSTM, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# Suppress warnings for clean output\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:30:41.698308Z","iopub.execute_input":"2026-01-31T23:30:41.698958Z","iopub.status.idle":"2026-01-31T23:31:11.574998Z","shell.execute_reply.started":"2026-01-31T23:30:41.698933Z","shell.execute_reply":"2026-01-31T23:31:11.574245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2 - dataset path detection\nDATASET_PATH = \"/kaggle/input/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO\"  # << edit if needed\n\nif not os.path.exists(DATASET_PATH):\n    # auto-detect any folder in /kaggle/input containing 'deep' or 'fake'\n    base = \"/kaggle/input\"\n    DATASET_PATH = None\n    for d in os.listdir(base):\n        if any(x in d.lower() for x in [\"deep\", \"deepfake\", \"fake\"]):\n            candidate = os.path.join(base, d)\n            if os.path.isdir(candidate):\n                DATASET_PATH = candidate\n                break\n\nif DATASET_PATH is None:\n    raise FileNotFoundError(\"Couldn't auto-detect dataset. Upload your dataset to Kaggle and set DATASET_PATH.\")\n\n# verify structure (expect REAL and FAKE folders inside)\nexpected = [os.path.join(DATASET_PATH, sub) for sub in [\"REAL\", \"FAKE\"]]\nfor path in expected:\n    if not os.path.isdir(path):\n        print(f\" Warning: expected subfolder not found: {path}\")\n\nprint(\"Using dataset path:\", DATASET_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:31:16.082629Z","iopub.execute_input":"2026-01-31T23:31:16.083707Z","iopub.status.idle":"2026-01-31T23:31:16.094285Z","shell.execute_reply.started":"2026-01-31T23:31:16.083675Z","shell.execute_reply":"2026-01-31T23:31:16.093503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3 - helpers\ndef infer_label_from_path(path):\n    \"\"\"Infer label from file path: 0 = real, 1 = fake, None = unknown\"\"\"\n    lower = path.lower()\n    parent = os.path.basename(os.path.dirname(path)).lower()\n\n    if \"real\" in parent or \"real\" in lower:\n        return 0\n    if any(x in parent for x in [\"fake\", \"deepfake\", \"synth\"]) or \\\n       any(x in lower for x in [\"fake\", \"deepfake\", \"synth\"]):\n        return 1\n    return None\n\n\ndef extract_mfcc_chunks(file_path,\n                        sr=22050,\n                        chunk_duration=1.0,\n                        n_mfcc=40,\n                        n_fft=2048,\n                        hop_length=512,\n                        max_pad_len=44):\n    \"\"\"\n    Load audio, split into chunks, compute MFCCs.\n    Returns: array (n_chunks, max_pad_len, n_mfcc)\n    \"\"\"\n    try:\n        y, _ = librosa.load(file_path, sr=sr, mono=True)\n    except Exception as e:\n        print(f\" Error loading {file_path}: {e}\")\n        return np.zeros((0, max_pad_len, n_mfcc), dtype=np.float32)\n\n    samples_per_chunk = int(chunk_duration * sr)\n    mfcc_chunks = []\n\n    for start in range(0, len(y), samples_per_chunk):\n        chunk = y[start:start + samples_per_chunk]\n        if len(chunk) == 0:\n            continue\n        if len(chunk) < samples_per_chunk:\n            chunk = np.pad(chunk, (0, samples_per_chunk - len(chunk)))\n\n        mfcc = librosa.feature.mfcc(y=chunk, sr=sr, n_mfcc=n_mfcc,\n                                    n_fft=n_fft, hop_length=hop_length).T\n\n        # pad/truncate to fixed length\n        mfcc = (mfcc[:max_pad_len, :]\n                if mfcc.shape[0] >= max_pad_len\n                else np.pad(mfcc, ((0, max_pad_len - mfcc.shape[0]), (0, 0))))\n\n        mfcc_chunks.append(mfcc)\n\n    if not mfcc_chunks:\n        return np.zeros((0, max_pad_len, n_mfcc), dtype=np.float32)\n\n    return np.array(mfcc_chunks, dtype=np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:31:26.100694Z","iopub.execute_input":"2026-01-31T23:31:26.101018Z","iopub.status.idle":"2026-01-31T23:31:26.109409Z","shell.execute_reply.started":"2026-01-31T23:31:26.100994Z","shell.execute_reply":"2026-01-31T23:31:26.108649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 - collect and extract features\nfrom joblib import Parallel, delayed\n\n# Controls\nCHUNK_DURATION = 1.0   # seconds\nN_MFCC = 40\nMAX_PAD_LEN = 44       # frames per 1s chunk\nMAX_SAMPLES_PER_CLASS = None  # set to int for limit, None = all\n\n# Gather labeled audio paths\nall_audio_paths = []\nfor root, _, files in os.walk(DATASET_PATH):\n    for f in files:\n        if f.lower().endswith(('.wav', '.flac', '.mp3', '.ogg')):\n            p = os.path.join(root, f)\n            label = infer_label_from_path(p)\n            if label is not None:\n                all_audio_paths.append((p, label))\n\nprint(\"Total labeled audio files found:\", len(all_audio_paths))\n\n# Balance / limit samples\nreal_paths = [p for p, l in all_audio_paths if l == 0]\nfake_paths = [p for p, l in all_audio_paths if l == 1]\n\nif MAX_SAMPLES_PER_CLASS:\n    real_paths = real_paths[:MAX_SAMPLES_PER_CLASS]\n    fake_paths = fake_paths[:MAX_SAMPLES_PER_CLASS]\n\nlabeled_files = [(p, 0) for p in real_paths] + [(p, 1) for p in fake_paths]\nrandom.shuffle(labeled_files)  # shuffle before extraction\nprint(\"Using files -> real:\", len(real_paths), \"fake:\", len(fake_paths))\n\n# Parallel MFCC extraction\ndef process_file(p, label):\n    try:\n        chunks = extract_mfcc_chunks(p,\n                                     chunk_duration=CHUNK_DURATION,\n                                     n_mfcc=N_MFCC,\n                                     max_pad_len=MAX_PAD_LEN)\n        if chunks.shape[0] > 0:\n            return chunks, [label] * chunks.shape[0]\n    except Exception as e:\n        print(f\" Error in {p}: {e}\")\n    return None\n\nresults = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n    delayed(process_file)(p, label) for p, label in tqdm(labeled_files, desc=\"Extracting MFCC chunks\")\n)\n\n# Collect results\nX_chunks, y_chunks = [], []\nfor r in results:\n    if r:\n        chunks, labels = r\n        X_chunks.append(chunks)\n        y_chunks.extend(labels)\n\nif not X_chunks:\n    raise RuntimeError(\"No chunks extracted. Check dataset & parameters.\")\n\n# Stack arrays\nX = np.vstack(X_chunks)   # (total_chunks, max_pad_len, n_mfcc)\ny = np.array(y_chunks)\nprint(\"Final dataset shape (chunks):\", X.shape, y.shape)\n\n# Save for reuse\nnp.save('X_mfcc_chunks.npy', X)\nnp.save('y_mfcc_chunks.npy', y)\nprint(\" Saved X_mfcc_chunks.npy and y_mfcc_chunks.npy\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:31:35.180758Z","iopub.execute_input":"2026-01-31T23:31:35.181064Z","iopub.status.idle":"2026-01-31T23:35:01.350984Z","shell.execute_reply.started":"2026-01-31T23:31:35.181042Z","shell.execute_reply":"2026-01-31T23:35:01.350200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5 - scale, split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\n# Reshape to (total_frames, n_mfcc) for scaling\nn_samples, time_steps, n_mfcc = X.shape\nX_flat = X.reshape(-1, n_mfcc)\n\n# Scale per coefficient\nscaler = StandardScaler(copy=False)\nX_flat = scaler.fit_transform(X_flat)\nX = X_flat.reshape(n_samples, time_steps, n_mfcc)\n\n# Stratified split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=SEED\n)\n\nprint(\"Train:\", X_train.shape, y_train.shape, \"Test:\", X_test.shape, y_test.shape)\n\n# Check class balance\nunique, counts = np.unique(y_train, return_counts=True)\nprint(\"Class distribution (train):\", dict(zip(unique, counts)))\nunique, counts = np.unique(y_test, return_counts=True)\nprint(\"Class distribution (test):\", dict(zip(unique, counts)))\n\n# Save scaler for inference\njoblib.dump(scaler, 'scaler.joblib')\nprint(\" Saved scaler.joblib\")\n\n# Optional: save split datasets\n# np.savez_compressed(\"train_test_split.npz\", \n#                     X_train=X_train, y_train=y_train, \n#                     X_test=X_test, y_test=y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:35:06.717240Z","iopub.execute_input":"2026-01-31T23:35:06.718079Z","iopub.status.idle":"2026-01-31T23:35:07.705300Z","shell.execute_reply.started":"2026-01-31T23:35:06.718044Z","shell.execute_reply":"2026-01-31T23:35:07.704439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6 - model\ndef make_lstm_model(time_steps, n_mfcc,\n                    lstm_units=[128, 64],\n                    dense_units=32,\n                    dropout_rates=[0.3, 0.3, 0.2],\n                    lr=1e-4):\n    model = Sequential([\n        Masking(mask_value=0., input_shape=(time_steps, n_mfcc)),\n\n        LSTM(lstm_units[0], return_sequences=True),\n        BatchNormalization(),\n        Dropout(dropout_rates[0]),\n\n        LSTM(lstm_units[1]),\n        BatchNormalization(),\n        Dropout(dropout_rates[1]),\n\n        Dense(dense_units, activation='relu'),\n        BatchNormalization(),\n        Dropout(dropout_rates[2]),\n\n        Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr, amsgrad=True),\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\nmodel = make_lstm_model(time_steps, n_mfcc)\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:35:13.849773Z","iopub.execute_input":"2026-01-31T23:35:13.850280Z","iopub.status.idle":"2026-01-31T23:35:15.466099Z","shell.execute_reply.started":"2026-01-31T23:35:13.850254Z","shell.execute_reply":"2026-01-31T23:35:15.465522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7 - train\nbatch_size = 32\nepochs = 5\n\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n    ModelCheckpoint('best_lstm.h5', monitor='val_loss', save_best_only=True, verbose=1)\n]\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_split=0.1,   # better: split train/val earlier if dataset is small\n    epochs=epochs,\n    batch_size=batch_size,\n    shuffle=True,\n    callbacks=callbacks,\n    verbose=2\n)\n\n# Save history for later plotting\nnp.save(\"training_history.npy\", history.history)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:35:44.177904Z","iopub.execute_input":"2026-01-31T23:35:44.178614Z","iopub.status.idle":"2026-01-31T23:36:22.730826Z","shell.execute_reply.started":"2026-01-31T23:35:44.178591Z","shell.execute_reply":"2026-01-31T23:36:22.730200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8 - evaluate\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n\n# Predictions\npreds = model.predict(X_test, batch_size=64)\ny_pred = (preds.flatten() >= 0.5).astype(int)\n\n# Metrics\nprint(classification_report(y_test, y_pred, digits=4, zero_division=0))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, preds))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion matrix:\\n\", cm)\n\n# Plot normalized + raw CM\nimport seaborn as sns\nfig, axes = plt.subplots(1, 2, figsize=(10,4))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['real','fake'], yticklabels=['real','fake'], ax=axes[0])\naxes[0].set_title(\"Confusion Matrix (Counts)\")\naxes[0].set_xlabel(\"Predicted\")\naxes[0].set_ylabel(\"True\")\n\nsns.heatmap(cm/cm.sum(axis=1, keepdims=True), annot=True, fmt=\".2f\", cmap='Blues',\n            xticklabels=['real','fake'], yticklabels=['real','fake'], ax=axes[1])\naxes[1].set_title(\"Confusion Matrix (Normalized)\")\naxes[1].set_xlabel(\"Predicted\")\naxes[1].set_ylabel(\"True\")\n\nplt.tight_layout()\nplt.show()\n\n# Optional: ROC Curve\nfpr, tpr, _ = roc_curve(y_test, preds)\nplt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(y_test, preds):.4f}\")\nplt.plot([0,1], [0,1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\n# Save results\nnp.savez_compressed(\"eval_results.npz\", y_true=y_test, y_pred=y_pred, preds=preds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:36:31.536787Z","iopub.execute_input":"2026-01-31T23:36:31.537084Z","iopub.status.idle":"2026-01-31T23:36:33.932983Z","shell.execute_reply.started":"2026-01-31T23:36:31.537063Z","shell.execute_reply":"2026-01-31T23:36:33.932423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9 - save and inference helper\nmodel.save('lstm_deepfake_audio.h5')\nprint(\"Saved model: lstm_deepfake_audio.h5\")\n\ndef predict_file(path, model, scaler_path='scaler.joblib',\n                 chunk_duration=1.0, n_mfcc=N_MFCC, max_pad_len=MAX_PAD_LEN,\n                 threshold=0.5):\n    scaler = joblib.load(scaler_path)\n    chunks = extract_mfcc_chunks(path, chunk_duration=chunk_duration,\n                                 n_mfcc=n_mfcc, max_pad_len=max_pad_len)\n    if chunks.shape[0] == 0:\n        return None, None, \"No audio chunks extracted\"\n    \n    # scale features\n    flat = chunks.reshape(-1, chunks.shape[-1])\n    flat = scaler.transform(flat)\n    chunks = flat.reshape(-1, chunks.shape[1], chunks.shape[2])\n    \n    preds = model.predict(chunks, verbose=0)\n    avg_score = preds.mean()\n    label = \"FAKE\" if avg_score >= threshold else \"REAL\"\n    return avg_score, preds, label\n\n# Example usage:\n# avg_score, per_chunk, label = predict_file(\"/kaggle/working/test.wav\", model)\n# print(f\"Prediction: {label} (avg_score={avg_score:.4f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:36:45.162689Z","iopub.execute_input":"2026-01-31T23:36:45.163794Z","iopub.status.idle":"2026-01-31T23:36:45.235639Z","shell.execute_reply.started":"2026-01-31T23:36:45.163764Z","shell.execute_reply":"2026-01-31T23:36:45.234870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_folder(folder_path, model, scaler_path='scaler.joblib',\n                   chunk_duration=1.0, n_mfcc=N_MFCC, max_pad_len=MAX_PAD_LEN,\n                   threshold=0.5):\n    results = []\n    for root, _, files in os.walk(folder_path):\n        for f in files:\n            if f.lower().endswith(('.wav', '.flac', '.mp3', '.ogg')):\n                file_path = os.path.join(root, f)\n                avg_score, preds, label = predict_file(file_path, model,\n                                                       scaler_path=scaler_path,\n                                                       chunk_duration=chunk_duration,\n                                                       n_mfcc=n_mfcc,\n                                                       max_pad_len=max_pad_len,\n                                                       threshold=threshold)\n                if avg_score is not None:\n                    results.append((file_path, avg_score, label))\n                    print(f\"{file_path} â†’ {label} (avg_score={avg_score:.4f})\")\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:37:03.294195Z","iopub.execute_input":"2026-01-31T23:37:03.294517Z","iopub.status.idle":"2026-01-31T23:37:03.300395Z","shell.execute_reply.started":"2026-01-31T23:37:03.294494Z","shell.execute_reply":"2026-01-31T23:37:03.299756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ”¹ Predict on multiple folders\nfolders = [\n    \"/kaggle/input/deep-voice-deepfake-voice-recognition/DEMONSTRATION/DEMONSTRATION\",\n    \"/kaggle/input/deep-voice-deepfake-voice-recognition/KAGGLE\"\n]\n\nall_results = []\nfor folder in folders:\n    print(\"\\nRunning predictions on:\", folder)\n    res = predict_folder(folder, model)\n    all_results.extend(res)\n\n# Save combined results\nimport pandas as pd\ndf_results = pd.DataFrame(all_results, columns=[\"file\", \"avg_score\", \"label\"])\ndf_results.to_csv(\"all_predictions.csv\", index=False)\nprint(\"\\n Saved predictions to all_predictions.csv\")\n\n\n# ðŸ”¹ Handle the CSV dataset (if it has file paths)\nimport pandas as pd\n\ncsv_path = \"/kaggle/input/deep-voice-deepfake-voice-recognition/KAGGLE/DATASET-balanced.csv\"\ndf = pd.read_csv(csv_path)\nprint(\"CSV loaded with shape:\", df.shape)\n\n# If the CSV contains a column with file paths (e.g., \"path\" or \"file\")\nif \"path\" in df.columns:\n    print(\"\\nRunning predictions from CSV paths...\")\n    csv_results = []\n    for p in df[\"path\"]:\n        avg_score, preds, label = predict_file(p, model, scaler_path=\"scaler.joblib\")\n        if avg_score is not None:\n            csv_results.append((p, avg_score, label))\n    pd.DataFrame(csv_results, columns=[\"file\", \"avg_score\", \"label\"]).to_csv(\"csv_predictions.csv\", index=False)\n    print(\" Saved predictions to csv_predictions.csv\")\nelse:\n    print(\" CSV doesnâ€™t contain file paths column. Please check its columns:\", df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T23:37:37.228037Z","iopub.execute_input":"2026-01-31T23:37:37.228338Z","iopub.status.idle":"2026-01-31T23:41:10.362137Z","shell.execute_reply.started":"2026-01-31T23:37:37.228316Z","shell.execute_reply":"2026-01-31T23:41:10.361266Z"}},"outputs":[],"execution_count":null}]}